---
title: "Topic Modelling"
output: html_notebook
---

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Setup -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->
# Setup
```{r custom themes}
custom_column_basic_theme <- function () { 
    theme_minimal(base_size=12, base_family="Avenir") +
        theme(
            panel.background  = element_blank(),
            panel.grid.minor = element_blank(),
            legend.position="none", # no legend
            plot.title = element_text(size = 12, hjust = 0),
            plot.subtitle = element_text(size = 10, hjust = 0),
            axis.title.y = element_text(size = 10),
            axis.title.x = element_text(size = 10),
            axis.text.x = element_text(size = 6)
        )
}
```

```{r import libraries}
library(tidytext)
library(lubridate)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(topicmodels) # topic modelling
library(ldatuning) # lda tuning for topic modelling
library(reshape2)
library(vader) # sentiment analysis
```
```{r global (static) variables}
TOP_WORDS <- 10 # number of top words to show when plotting topics
TUNING <- FALSE # NOTE: set to TRUE to run ldatuning

# LDA tuning number of topic variables
NUM_TOPICS_3 <- 3
NUM_TOPICS_4 <- 4 # all outlet topics per month have this number of topics
NUM_TOPICS_5 <- 5
NUM_TOPICS_6 <- 6
NUM_TOPICS_7 <- 7
```

```{r read data}
tweets <- read.csv("data/cleaned_tweets.csv") %>%
  mutate(created_at = as.Date(created_at))
```

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Data Wrangling: Topic modelling -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->
# Data Wrangling: Topic Modelling
## Data Cleaning already done
```{r tidy tweets}
clean_tweets <- function(tweets_in) {
  # remove stopwords, links, mentions, special characters, and numbers
  tidy_tweets <- tweets_in %>% 
    unnest_tokens(word, cleaned_tweet, token = "tweets")

  return(tidy_tweets)
}

tidy_tweets <- clean_tweets(tweets)
```

## Topic Modelling Setup

```{r extract top words per topic}
# topics arg: df containing word, topic and beta (probability that the word occurs in that topic) values
# words_per_topic: number of top words to get
top_words_per_topic <- function(topics, words_per_topic) {
  return(topics %>%
  group_by(topic) %>%
  slice_max(beta, n = words_per_topic) %>% # get most relevant words for that topic
  ungroup() %>%
  arrange(topic, -beta)) # arrange words with decreasing relevance to the topic
}
```


```{r LDA tuning}
# to find optimal number of topics to model for given document term matrix
# source: https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html
lda_tuning <- function(orgs_dtm) {
  result <- FindTopicsNumber(
      orgs_dtm,
      topics = seq(from = 2, to = 8, by = 1),
      metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
      method = "Gibbs",
      control = list(seed = 77),
      mc.cores = 2L,
      verbose = TRUE
    )
  # choose extreme in following plots as optimal number of topics
  FindTopicsNumber_plot(result)
}
```

```{r extract topics using LDA output}
get_topics <- function(word_counts, nr_topics) {
  # create document term matrix from word count
  orgs_dtm <- word_counts %>%
    cast_dtm(corpus, word, n)
  
  # checking for ideal number of topics (NOTE: set to TRUE to run ldatuning)
  if (TUNING) {
    lda_tuning(orgs_dtm)
  }
  
  # apply topic modelling to dtm and extract relevant data from LDA output
  orgs_lda <- LDA(orgs_dtm, k = nr_topics, control = list(seed = 1234))
  return(tidy(orgs_lda, matrix = "beta"))
}
```


## Overall topics

```{r apply topic modelling to get overall topics}
overall_topics <- function(tidy_tweets_in, nr_topics) {
  # word count by tweet
  # TODO: check whether it should be by tweet or by user (for data analysis proj it was media org)
  word_counts <- tidy_tweets_in %>%
    select(corpus, word, created_at) %>%
    count(corpus, word, sort = TRUE) %>% # count how often each org uses each word
    ungroup()
  
  orgs_topics_overall <- get_topics(word_counts, nr_topics)
  
  return(orgs_topics_overall)
}
```

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Data Visualisation: Topic modelling -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

# Data Visualisation: Topic Modelling

## Overall topics

```{r plot topics}
plot_overall_topics <- function(tidy_tweets_in, nr_topics) {
  
  # get the topics to plot
  orgs_topics <- overall_topics(tidy_tweets_in, nr_topics)
  
  # top words per topic
  top_terms <- top_words_per_topic(orgs_topics, TOP_WORDS) %>%
    mutate(term = reorder_within(term, beta, topic)) %>% # order terms to decreasing relevance
    group_by(topic) %>%
    filter(row_number()<=TOP_WORDS) # force number of terms to be TOP_WORDS per topic
  
  # plot top terms per topic (sorted by beta relevance)
  topics_plot <- top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    labs(title = paste("NFT-related Topics")) +
    facet_wrap(~ topic, scales = "free") + # combine plot for each topic
    scale_y_reordered() +
    custom_column_basic_theme()
  # save plot as jpeg
  ggsave(paste("model_for_", nr_topics, "_topics.jpeg", sep = ""), plot = topics_plot) 
}
```

```{r plot overall topics using selected number of topics (lda tuning)}
plot_overall_topics(tidy_tweets, NUM_TOPICS_7)
```