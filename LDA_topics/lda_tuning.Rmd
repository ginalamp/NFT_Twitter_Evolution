---
title: "Topic Modelling"
output: html_notebook
---

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Setup -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->
# Setup
```{r custom themes}
custom_column_basic_theme <- function () { 
    theme_minimal(base_size=12, base_family="Avenir") +
        theme(
            panel.background  = element_blank(),
            panel.grid.minor = element_blank(),
            legend.position="none", # no legend
            plot.title = element_text(size = 12, hjust = 0),
            plot.subtitle = element_text(size = 10, hjust = 0),
            axis.title.y = element_text(size = 10),
            axis.title.x = element_text(size = 10),
            axis.text.x = element_text(size = 6)
        )
}
```

```{r import libraries}
library(tidytext)
library(lubridate)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(topicmodels) # topic modelling
library(ldatuning) # lda tuning for topic modelling
library(reshape2)
library(vader) # sentiment analysis
```
```{r global (static) variables}
LOWER <- 2 # lower bound for range of topic numbers to check
UPPER <- 8 # upper bound for range of topic numbers to check
```

```{r read data}
tweets <- read.csv("data/cleaned_tweets.csv") %>%
  mutate(created_at = as.Date(created_at))
```

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Data Wrangling: Topic modelling -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->
# Data Wrangling: Topic Modelling
## Data Cleaning already done
```{r tidy tweets}
clean_tweets <- function(tweets_in) {
  # remove stopwords, links, mentions, special characters, and numbers
  tidy_tweets <- tweets_in %>% 
    unnest_tokens(word, cleaned_tweet, token = "tweets")

  return(tidy_tweets)
}

tidy_tweets <- clean_tweets(tweets)
```

## Topic Modelling Setup

```{r extract top words per topic}
# topics arg: df containing word, topic and beta (probability that the word occurs in that topic) values
# words_per_topic: number of top words to get
top_words_per_topic <- function(topics, words_per_topic) {
  return(topics %>%
  group_by(topic) %>%
  slice_max(beta, n = words_per_topic) %>% # get most relevant words for that topic
  ungroup() %>%
  arrange(topic, -beta)) # arrange words with decreasing relevance to the topic
}
```

```{r LDA tuning}
x_lda_tuning <- function(tidy_tweets_in) {
  # word count by tweet
  # TODO: check whether it should be by tweet or by user (for data analysis proj it was media org)
  word_counts <- tidy_tweets_in %>%
    select(corpus, word, created_at) %>%
    count(corpus, word, sort = TRUE) %>% # count how often each org uses each word
    ungroup()
  
  # create document term matrix from word count
  orgs_dtm <- word_counts %>%
    cast_dtm(corpus, word, n)
  
  # checking for ideal number of topics
  result <- FindTopicsNumber(
      orgs_dtm,
      topics = seq(from = LOWER, to = UPPER, by = 1),
      metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
      method = "Gibbs",
      control = list(seed = 77),
      mc.cores = 2L,
      verbose = TRUE
    )
  # choose extreme in following plots as optimal number of topics
  # plot saved as jpeg
  jpeg(file=paste("tuning_range_", LOWER, "_", UPPER, ".jpeg", sep = ""))
  FindTopicsNumber_plot(result)
  dev.off()
}

x_lda_tuning(tidy_tweets)
```

