{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e6de0",
   "metadata": {},
   "source": [
    "# Merge sentiment graphs\n",
    "Not in script format yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826aebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Outputs overall sentiment (with rounded polarity) and sentiment over time (frequency bins).\n",
    "    Also computes overall average sentiment.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates # plot sentiment over time\n",
    "import seaborn as sns\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b01a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall\n",
    "NUM_SEGMENTS = 34 # decided on 34 segments for overall data\n",
    "# Input/output files for overall data\n",
    "DATA_IN = \"../datain/sentiment/cleaned_tweets_for_sentiment.csv\"\n",
    "ROUNDED_POLARITY_OUT = \"../dataout/sentiment/rounded_sentiment_overall.jpeg\"\n",
    "SENTIMENT_OVER_TIME_PER_SEGMENT_OUT = '../dataout/sentiment/sentiment_per_segment_overall.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3810ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2792751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentiment_data(df):\n",
    "    '''\n",
    "        Load & clean data.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe containing the sentiment data\n",
    "        Returns:\n",
    "            df: cleaned dataframe\n",
    "    '''\n",
    "    # remove all null created_at values from dataframe\n",
    "    df = df.drop(df[df['created_at'].isnull()].index)\n",
    "    df = df.drop(df[df['cleaned_tweet'].isnull()].index)\n",
    "    # ensure that all values in created_at has 2021 (and not random strings)\n",
    "    df = df[df['created_at'].str.contains(\"2021\")]\n",
    "\n",
    "    # split created_at into date and time columns\n",
    "    # https://intellipaat.com/community/13909/python-how-can-i-split-a-column-with-both-date-and-time-e-g-2019-07-02-00-12-32-utc-into-two-separate-columns\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['date'] = df['created_at'].dt.date\n",
    "    df['time'] = df['created_at'].dt.time\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def sentiment_polarity_score(df, overall=True, selected_topic=0, filename=ROUNDED_POLARITY_OUT):\n",
    "    '''\n",
    "        Calculates the sentiment polarity score.\n",
    "\n",
    "        Args:\n",
    "            df: cleaned dataframe with tweet data\n",
    "            overall: boolean (true if want to analyse overall data frequency, false if not)\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "            filename: path to the file to which this function will output to.\n",
    "        Returns:\n",
    "            df: dataframe with Vader sentiment polarity score columns added.\n",
    "    '''\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # add polarity scores to df\n",
    "    # https://github.com/sidneykung/twitter_hate_speech_detection/blob/master/preprocessing/VADER_sentiment.ipynb\n",
    "    print(f\"\\t\\tGetting sentiment polarity scores...\")\n",
    "    pol = lambda x: analyzer.polarity_scores(x)\n",
    "    df['polarity'] = df[\"cleaned_tweet\"].progress_apply(pol)\n",
    "\n",
    "    # split polarity scores into separate columns\n",
    "    print(f\"\\t\\tSplitting polarity scores into columns...\")\n",
    "    df = pd.concat([df.drop(['polarity'], axis=1), df['polarity'].progress_apply(pd.Series)], axis=1)\n",
    "\n",
    "    # get rounded polarity score\n",
    "    round_pol = lambda x: calc_polarity(x, 0.05)\n",
    "    # round polarity up/down\n",
    "    df['rounded_polarity'] = df['compound'].apply(round_pol)\n",
    "\n",
    "    # get amount of rounded negative, neutral, and positive polarity\n",
    "    num_rounded_sentiments = df.groupby('rounded_polarity').count()\n",
    "    plot_rounded_polarity(num_rounded_sentiments, overall, selected_topic, filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_polarity(x, bound):\n",
    "    '''\n",
    "        Round polarity up/down based on bound.\n",
    "\n",
    "        Args:\n",
    "            x: \n",
    "            bound:\n",
    "        Returns:\n",
    "            int: -1 if x is less than -bound, 1 greater than bound, or 0\n",
    "    '''\n",
    "    if x < -bound:\n",
    "        return -1\n",
    "    elif x > bound:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def plot_rounded_polarity(num_rounded_sentiments, overall, selected_topic, filename):\n",
    "    '''\n",
    "        Plot rounded polariry.\n",
    "        Called by sentiment_polarity_score().\n",
    "\n",
    "        Args:\n",
    "            num_rounded_sentiments: dataframe grouped by rounded polarity\n",
    "            overall: boolean (true if want to analyse overall data frequency, false if not)\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "            filename: path to the file to which this function will output to.\n",
    "    '''\n",
    "    # plot rounded negative, neutral, and positive sentiment amounts\n",
    "    plt.bar(num_rounded_sentiments.index, num_rounded_sentiments[\"compound\"])\n",
    "    if overall:\n",
    "        plt.title('Overall Rounded Sentiment')\n",
    "    else:\n",
    "        plt.title(f'Topic {selected_topic} Rounded Sentiment')\n",
    "\n",
    "    plt.xlabel('Polarity')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def split_data_segments(df, num_segments=NUM_SEGMENTS):\n",
    "    '''\n",
    "        Split data into segments according to date.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe with Vader sentiment polarity score columns added.\n",
    "            num_segments: number of equal segments that the data needs to be split into.\n",
    "        Returns:\n",
    "            df: sorted df by date\n",
    "            sub_dfs: a list of subdataframes of df\n",
    "            num_segments: number of equal segments that the data needs to be split into.\n",
    "    '''\n",
    "    # sort dataframe by date\n",
    "    df = df.sort_values(by=['date', 'time'])\n",
    "    # list of dfs\n",
    "    sub_dfs = list(split(df, num_segments))\n",
    "    return df, sub_dfs, num_segments\n",
    "\n",
    "\n",
    "def split(df, n):\n",
    "    '''\n",
    "        Split df into n groups of equal length (returns list of sub dataframes).\n",
    "        https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length\n",
    "\n",
    "        Args:\n",
    "            df: dataframe that should be split\n",
    "            n: number of equal segments that the data needs to be split into.\n",
    "        Retuns:\n",
    "            sub dataframe according to df and n\n",
    "    '''\n",
    "    k, m = divmod(len(df), n)\n",
    "    return (df[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "\n",
    "def sentiment_per_segment(df, sub_dfs, num_segments, num_tweets_per_segment, overall=True, selected_topic=0, filename=SENTIMENT_OVER_TIME_PER_SEGMENT_OUT):\n",
    "    '''\n",
    "        Get average sentiment & plot sentiment over time.\n",
    "\n",
    "        Args:\n",
    "            df: sorted df by date\n",
    "            sub_dfs: a list of subdataframes of df\n",
    "            num_segments: number of equal segments that the data needs to be split into.\n",
    "            num_tweets_per_segment: number of tweets per segment.\n",
    "            overall: boolean (true if want to analyse overall data frequency, false if not)\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "            filename: path to the file to which this function will output to.\n",
    "        Returns:\n",
    "            avg_sentiment: the average sentiment over the entire timeperiod for the data.\n",
    "    '''\n",
    "    compounds = []\n",
    "    mns, mxs = [], []\n",
    "    dates = []\n",
    "    for sub_df in sub_dfs:\n",
    "        compounds.append(sub_df.compound.mean())\n",
    "        mxs.append(sub_df.index.max())\n",
    "        mns.append(sub_df.index.min())\n",
    "        dates.append(sub_df.date.iloc[0])\n",
    "\n",
    "    compound_df = pd.DataFrame(dict(\n",
    "        mn=mns,\n",
    "        mx=mxs,\n",
    "        compouned=compounds,\n",
    "        date=dates,\n",
    "    ))\n",
    "    \n",
    "    return compound_df, num_segments, num_tweets_per_segment, overall, selected_topic, filename\n",
    "\n",
    "#     plot_sentiment_over_time(compound_df, num_segments, num_tweets_per_segment, overall, selected_topic, filename)\n",
    "    \n",
    "#     # average overall sentiment\n",
    "#     avg_sentiment = df['compound'].mean()\n",
    "#     return avg_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b75466",
   "metadata": {},
   "source": [
    "# Get overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc05503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying overall sentiment analysis on segments over time...\n",
      "\t\tGetting sentiment polarity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 406565/406565 [00:48<00:00, 8391.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSplitting polarity scores into columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 406565/406565 [02:04<00:00, 3261.76it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sx/4r548nsj58s0_cc4jj_vs37h0000gn/T/ipykernel_11912/467653008.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_tweets_per_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moverall_compound_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_num_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_num_tweets_per_segment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_per_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tweets_per_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tAverage sentiment overall is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Applying overall sentiment analysis on segments over time...\")\n",
    "# load cleaned tweet corpus data\n",
    "df = pd.read_csv(DATA_IN)\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df = clean_sentiment_data(df)\n",
    "\n",
    "df = sentiment_polarity_score(df)\n",
    "# segments\n",
    "df, sub_dfs, num_segments = split_data_segments(df)\n",
    "num_tweets_per_segment = round(len(sub_dfs[0]) / 1000, 1)\n",
    "overall_compound_df, overall_num_segments, overall_num_tweets_per_segment, overall, selected_topic, filename = sentiment_per_segment(df, sub_dfs, num_segments, num_tweets_per_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca0a56",
   "metadata": {},
   "source": [
    "# Get largest topic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2432f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_DATA_IN_PREFIX = \"../datain/sentiment/\"\n",
    "SENTIMENT_DATA_OUT_PREFIX = \"../dataout/sentiment/\"\n",
    "BTM_SCORES_DATA_IN = \"../BTM_topics/dataout/\"\n",
    "BTM_DATA_IN_PREFIX = \"../datain/topic_modelling/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc5ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(optimal_num_topics=11):\n",
    "    '''\n",
    "        Get data.\n",
    "\n",
    "        Args:\n",
    "            optimal_num_topics: optimal number of topics identified by the ElbowMethod (using the R BTM LogLik values)\n",
    "        Returns\n",
    "            df: loaded BTM scores dataframe\n",
    "    '''\n",
    "    filename = BTM_SCORES_DATA_IN + f\"{optimal_num_topics}_model_scores.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # change index to id\n",
    "    df = df.rename({'Unnamed: 0': 'id'}, axis=1) # rename column\n",
    "    df['id'] = df['id'].astype('int64')\n",
    "    df.set_index(\"id\", inplace = True)\n",
    "\n",
    "    # rename column headers to integer representations\n",
    "    for i in range(1, len(df.columns) + 1):\n",
    "        colname = \"V\" + str(i)\n",
    "        df = df.rename({colname: i}, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def match_topic_with_tweet(df):\n",
    "    '''\n",
    "        Get the topic that a tweet is most likely part of based on the \n",
    "        probablity that they're in the topic.\n",
    "\n",
    "        Args:\n",
    "            df: loaded BTM scores dataframe\n",
    "        Returns:\n",
    "            df: df with a column indicating their most probable topic\n",
    "    '''\n",
    "    maxtopic = df\n",
    "    # get the topic with the max probability value for each row\n",
    "    maxtopic = maxtopic.idxmax(axis=1)\n",
    "    # convert all topics from string ('15') to int (15). This prerpares it for grouping by topic\n",
    "    maxtopic = maxtopic.astype(int)\n",
    "\n",
    "    # add maxtopic as a new column\n",
    "    df.insert(0, \"maxtopic\", maxtopic)\n",
    "\n",
    "    # sort by maxtopic\n",
    "    df = df.sort_values('maxtopic')\n",
    "\n",
    "    return df\n",
    "\n",
    "def sentiment_get_matching_topic_data(selected_topic):\n",
    "    '''\n",
    "        Get the subset of the topic modelling data from the cleaned sentiment data \n",
    "        (use the topic IDs to get the sentiment data matching those ids).\n",
    "\n",
    "        Args:\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "        Returns:\n",
    "            selected_topic_sentiment_df: subset of cleaned sentiment data that matches the selected topic's tweet ids.\n",
    "    '''\n",
    "    filename = SENTIMENT_DATA_IN_PREFIX + \"cleaned_tweets_for_sentiment.csv\"\n",
    "    # load cleaned tweet corpus data\n",
    "    cleaned_sentiment_df = pd.read_csv(filename)\n",
    "    cleaned_sentiment_df = cleaned_sentiment_df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "    # load topic ids\n",
    "    filename = SENTIMENT_DATA_IN_PREFIX + f\"ids_topic_{selected_topic}.csv\"\n",
    "    selected_topic_ids = pd.read_csv(filename)\n",
    "\n",
    "    # subset sentiment data with topic ids\n",
    "    selected_topic_sentiment_df = selected_topic_ids.merge(cleaned_sentiment_df, on='id', how='left')\n",
    "\n",
    "    # export selected topic sentiment to csv\n",
    "    filename = BTM_DATA_IN_PREFIX + f\"tweet_sentiment_subdf_topic_{selected_topic}.csv\"\n",
    "    selected_topic_sentiment_df.to_csv(filename)\n",
    "\n",
    "    return selected_topic_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5caa2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 40\n",
    "optimal_num_topics = 11\n",
    "df = load_data(optimal_num_topics)\n",
    "df = match_topic_with_tweet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b57d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_topic = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5617ee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGetting topic sentiment...\n",
      "\t\tGetting sentiment polarity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 100879/100879 [00:14<00:00, 6995.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSplitting polarity scores into columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 100879/100879 [00:31<00:00, 3159.14it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tGetting topic sentiment...\")\n",
    "# sentiment analysis\n",
    "df = sentiment_get_matching_topic_data(selected_topic)\n",
    "df = clean_sentiment_data(df)\n",
    "filename = SENTIMENT_DATA_OUT_PREFIX + f\"rounded_sentiment_topic_{selected_topic}.pdf\"\n",
    "\n",
    "df = sentiment_polarity_score(df, False, selected_topic, filename)\n",
    "# segments\n",
    "df, sub_dfs, num_segments = split_data_segments(df, NUM_SEGMENTS)\n",
    "num_tweets_per_segment = round(len(sub_dfs[0]) / 1000, 1)\n",
    "filename = SENTIMENT_DATA_OUT_PREFIX + f\"sentiment_per_segment_topic_{selected_topic}.pdf\"\n",
    "lt_compound_df, lt_num_segments, lt_num_tweets_per_segment, overall, selected_topic, filename = sentiment_per_segment(df, sub_dfs, num_segments, num_tweets_per_segment, False, selected_topic, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c07850",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27c26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_over_time(overall_compound_df, overall_num_segments, overall_num_tweets_per_segment, lt_compound_df, lt_num_segments, lt_num_tweets_per_segment, selected_topic, filename):\n",
    "    '''\n",
    "        Plot sentiment over time.\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(overall_compound_df.date, 'compouned', data=overall_compound_df, label=\"Overall\")\n",
    "    ax.plot(lt_compound_df.date, 'compouned', data=lt_compound_df, label=f\"Topic {selected_topic}\")\n",
    "\n",
    "    # Major ticks every month.\n",
    "    fmt_month = mdates.MonthLocator()\n",
    "    ax.xaxis.set_major_locator(fmt_month)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "    # plot\n",
    "    plt.title(f'Overall and Topic {selected_topic} Sentiment per segment')\n",
    "    \n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Vader Sentiment score')\n",
    "    \n",
    "    plt.figtext(0.14,-0.05,f'Overall: {overall_num_segments} segments of ~{overall_num_tweets_per_segment}k, Topic {selected_topic}: {lt_num_segments} segments of ~{lt_num_tweets_per_segment}k')\n",
    "    # save graph\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7ae9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = SENTIMENT_DATA_OUT_PREFIX + f\"sentiment_per_segment_topic_overall_{selected_topic}.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d6511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_over_time(overall_compound_df, overall_num_segments, overall_num_tweets_per_segment, lt_compound_df, lt_num_segments, lt_num_tweets_per_segment, selected_topic, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
