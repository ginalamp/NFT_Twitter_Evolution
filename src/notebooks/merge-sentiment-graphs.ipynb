{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e6de0",
   "metadata": {},
   "source": [
    "# Merge sentiment graphs\n",
    "Not in script format yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826aebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Outputs overall sentiment (with rounded polarity) and sentiment over time (frequency bins).\n",
    "    Also computes overall average sentiment.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates # plot sentiment over time\n",
    "import seaborn as sns\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b01a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall\n",
    "NUM_SEGMENTS = 34 # decided on 34 segments for overall data\n",
    "# Input/output files for overall data\n",
    "DATA_IN = \"../datain/sentiment/cleaned_tweets_for_sentiment.csv\"\n",
    "ROUNDED_POLARITY_OUT = \"../dataout/sentiment/rounded_sentiment_overall.jpeg\"\n",
    "SENTIMENT_OVER_TIME_PER_SEGMENT_OUT = '../dataout/sentiment/sentiment_per_segment_overall.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3810ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2792751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentiment_data(df):\n",
    "    '''\n",
    "        Load & clean data.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe containing the sentiment data\n",
    "        Returns:\n",
    "            df: cleaned dataframe\n",
    "    '''\n",
    "    # remove all null created_at values from dataframe\n",
    "    df = df.drop(df[df['created_at'].isnull()].index)\n",
    "    df = df.drop(df[df['cleaned_tweet'].isnull()].index)\n",
    "    # ensure that all values in created_at has 2021 (and not random strings)\n",
    "    df = df[df['created_at'].str.contains(\"2021\")]\n",
    "\n",
    "    # split created_at into date and time columns\n",
    "    # https://intellipaat.com/community/13909/python-how-can-i-split-a-column-with-both-date-and-time-e-g-2019-07-02-00-12-32-utc-into-two-separate-columns\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['date'] = df['created_at'].dt.date\n",
    "    df['time'] = df['created_at'].dt.time\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def sentiment_polarity_score(df, overall=True, selected_topic=0, filename=ROUNDED_POLARITY_OUT):\n",
    "    '''\n",
    "        Calculates the sentiment polarity score.\n",
    "\n",
    "        Args:\n",
    "            df: cleaned dataframe with tweet data\n",
    "            overall: boolean (true if want to analyse overall data frequency, false if not)\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "            filename: path to the file to which this function will output to.\n",
    "        Returns:\n",
    "            df: dataframe with Vader sentiment polarity score columns added.\n",
    "    '''\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # add polarity scores to df\n",
    "    # https://github.com/sidneykung/twitter_hate_speech_detection/blob/master/preprocessing/VADER_sentiment.ipynb\n",
    "    print(f\"\\t\\tGetting sentiment polarity scores...\")\n",
    "    pol = lambda x: analyzer.polarity_scores(x)\n",
    "    df['polarity'] = df[\"cleaned_tweet\"].progress_apply(pol)\n",
    "\n",
    "    # split polarity scores into separate columns\n",
    "    print(f\"\\t\\tSplitting polarity scores into columns...\")\n",
    "    df = pd.concat([df.drop(['polarity'], axis=1), df['polarity'].progress_apply(pd.Series)], axis=1)\n",
    "\n",
    "    # get rounded polarity score\n",
    "    round_pol = lambda x: calc_polarity(x, 0.05)\n",
    "    # round polarity up/down\n",
    "    df['rounded_polarity'] = df['compound'].apply(round_pol)\n",
    "\n",
    "    # get amount of rounded negative, neutral, and positive polarity\n",
    "    num_rounded_sentiments = df.groupby('rounded_polarity').count()\n",
    "    plot_rounded_polarity(num_rounded_sentiments, overall, selected_topic, filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_polarity(x, bound):\n",
    "    '''\n",
    "        Round polarity up/down based on bound.\n",
    "\n",
    "        Args:\n",
    "            x: \n",
    "            bound:\n",
    "        Returns:\n",
    "            int: -1 if x is less than -bound, 1 greater than bound, or 0\n",
    "    '''\n",
    "    if x < -bound:\n",
    "        return -1\n",
    "    elif x > bound:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def plot_rounded_polarity(num_rounded_sentiments, overall, selected_topic, filename):\n",
    "    '''\n",
    "        Plot rounded polariry.\n",
    "        Called by sentiment_polarity_score().\n",
    "\n",
    "        Args:\n",
    "            num_rounded_sentiments: dataframe grouped by rounded polarity\n",
    "            overall: boolean (true if want to analyse overall data frequency, false if not)\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "            filename: path to the file to which this function will output to.\n",
    "    '''\n",
    "    # plot rounded negative, neutral, and positive sentiment amounts\n",
    "    plt.bar(num_rounded_sentiments.index, num_rounded_sentiments[\"compound\"])\n",
    "    if overall:\n",
    "        plt.title('Overall Rounded Sentiment')\n",
    "    else:\n",
    "        plt.title(f'Topic {selected_topic} Rounded Sentiment')\n",
    "\n",
    "    plt.xlabel('Polarity')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def split_data_segments(df, num_segments=NUM_SEGMENTS):\n",
    "    '''\n",
    "        Split data into segments according to date.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe with Vader sentiment polarity score columns added.\n",
    "            num_segments: number of equal segments that the data needs to be split into.\n",
    "        Returns:\n",
    "            df: sorted df by date\n",
    "            sub_dfs: a list of subdataframes of df\n",
    "            num_segments: number of equal segments that the data needs to be split into.\n",
    "    '''\n",
    "    # sort dataframe by date\n",
    "    df = df.sort_values(by=['date', 'time'])\n",
    "    # list of dfs\n",
    "    sub_dfs = list(split(df, num_segments))\n",
    "    return df, sub_dfs, num_segments\n",
    "\n",
    "\n",
    "def split(df, n):\n",
    "    '''\n",
    "        Split df into n groups of equal length (returns list of sub dataframes).\n",
    "        https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length\n",
    "\n",
    "        Args:\n",
    "            df: dataframe that should be split\n",
    "            n: number of equal segments that the data needs to be split into.\n",
    "        Retuns:\n",
    "            sub dataframe according to df and n\n",
    "    '''\n",
    "    k, m = divmod(len(df), n)\n",
    "    return (df[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "\n",
    "def sentiment_per_segment(df, sub_dfs, num_segments, num_tweets_per_segment, overall=True, selected_topic=0, filename=SENTIMENT_OVER_TIME_PER_SEGMENT_OUT):\n",
    "    '''\n",
    "        Get average sentiment & plot sentiment over time.\n",
    "\n",
    "        Args:\n",
    "            df: sorted df by date\n",
    "            sub_dfs: a list of subdataframes of df\n",
    "            num_segments: number of equal segments that the data needs to be split into.\n",
    "            num_tweets_per_segment: number of tweets per segment.\n",
    "            overall: boolean (true if want to analyse overall data frequency, false if not)\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "            filename: path to the file to which this function will output to.\n",
    "        Returns:\n",
    "            avg_sentiment: the average sentiment over the entire timeperiod for the data.\n",
    "    '''\n",
    "    compounds = []\n",
    "    mns, mxs = [], []\n",
    "    dates = []\n",
    "    for sub_df in sub_dfs:\n",
    "        compounds.append(sub_df.compound.mean())\n",
    "        mxs.append(sub_df.index.max())\n",
    "        mns.append(sub_df.index.min())\n",
    "        dates.append(sub_df.date.iloc[0])\n",
    "\n",
    "    compound_df = pd.DataFrame(dict(\n",
    "        mn=mns,\n",
    "        mx=mxs,\n",
    "        compouned=compounds,\n",
    "        date=dates,\n",
    "    ))\n",
    "    \n",
    "    return compound_df, num_segments, num_tweets_per_segment, overall, selected_topic, filename\n",
    "\n",
    "#     plot_sentiment_over_time(compound_df, num_segments, num_tweets_per_segment, overall, selected_topic, filename)\n",
    "    \n",
    "#     # average overall sentiment\n",
    "#     avg_sentiment = df['compound'].mean()\n",
    "#     return avg_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b75466",
   "metadata": {},
   "source": [
    "# Get overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc05503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying overall sentiment analysis on segments over time...\n",
      "\t\tGetting sentiment polarity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 406631/406631 [00:22<00:00, 18408.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSplitting polarity scores into columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 406631/406631 [00:39<00:00, 10305.48it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying overall sentiment analysis on segments over time...\")\n",
    "# load cleaned tweet corpus data\n",
    "df = pd.read_csv(DATA_IN)\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df = clean_sentiment_data(df)\n",
    "\n",
    "df = sentiment_polarity_score(df)\n",
    "# segments\n",
    "df, sub_dfs, num_segments = split_data_segments(df)\n",
    "num_tweets_per_segment = round(len(sub_dfs[0]) / 1000, 1)\n",
    "overall_compound_df, overall_num_segments, overall_num_tweets_per_segment, overall, selected_topic, filename = sentiment_per_segment(df, sub_dfs, num_segments, num_tweets_per_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca0a56",
   "metadata": {},
   "source": [
    "# Get largest topic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2432f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_DATA_IN_PREFIX = \"../datain/sentiment/\"\n",
    "SENTIMENT_DATA_OUT_PREFIX = \"../dataout/sentiment/\"\n",
    "BTM_SCORES_DATA_IN = \"../BTM_topics/dataout/\"\n",
    "BTM_DATA_IN_PREFIX = \"../datain/topic_modelling/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc5ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(optimal_num_topics=11):\n",
    "    '''\n",
    "        Get data.\n",
    "\n",
    "        Args:\n",
    "            optimal_num_topics: optimal number of topics identified by the ElbowMethod (using the R BTM LogLik values)\n",
    "        Returns\n",
    "            df: loaded BTM scores dataframe\n",
    "    '''\n",
    "    filename = BTM_SCORES_DATA_IN + f\"{optimal_num_topics}_model_scores.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # change index to id\n",
    "    df = df.rename({'Unnamed: 0': 'id'}, axis=1) # rename column\n",
    "    df['id'] = df['id'].astype('int64')\n",
    "    df.set_index(\"id\", inplace = True)\n",
    "\n",
    "    # rename column headers to integer representations\n",
    "    for i in range(1, len(df.columns) + 1):\n",
    "        colname = \"V\" + str(i)\n",
    "        df = df.rename({colname: i}, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def match_topic_with_tweet(df):\n",
    "    '''\n",
    "        Get the topic that a tweet is most likely part of based on the \n",
    "        probablity that they're in the topic.\n",
    "\n",
    "        Args:\n",
    "            df: loaded BTM scores dataframe\n",
    "        Returns:\n",
    "            df: df with a column indicating their most probable topic\n",
    "    '''\n",
    "    maxtopic = df\n",
    "    # get the topic with the max probability value for each row\n",
    "    maxtopic = maxtopic.idxmax(axis=1)\n",
    "    # convert all topics from string ('15') to int (15). This prerpares it for grouping by topic\n",
    "    maxtopic = maxtopic.astype(int)\n",
    "\n",
    "    # add maxtopic as a new column\n",
    "    df.insert(0, \"maxtopic\", maxtopic)\n",
    "\n",
    "    # sort by maxtopic\n",
    "    df = df.sort_values('maxtopic')\n",
    "\n",
    "    return df\n",
    "\n",
    "def sentiment_get_matching_topic_data(selected_topic):\n",
    "    '''\n",
    "        Get the subset of the topic modelling data from the cleaned sentiment data \n",
    "        (use the topic IDs to get the sentiment data matching those ids).\n",
    "\n",
    "        Args:\n",
    "            selected_topic: the topic number of the topic to be analysed.\n",
    "        Returns:\n",
    "            selected_topic_sentiment_df: subset of cleaned sentiment data that matches the selected topic's tweet ids.\n",
    "    '''\n",
    "    filename = SENTIMENT_DATA_IN_PREFIX + \"cleaned_tweets_for_sentiment.csv\"\n",
    "    # load cleaned tweet corpus data\n",
    "    cleaned_sentiment_df = pd.read_csv(filename)\n",
    "    cleaned_sentiment_df = cleaned_sentiment_df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "    # load topic ids\n",
    "    filename = SENTIMENT_DATA_IN_PREFIX + f\"ids_topic_{selected_topic}.csv\"\n",
    "    selected_topic_ids = pd.read_csv(filename)\n",
    "\n",
    "    # subset sentiment data with topic ids\n",
    "    selected_topic_sentiment_df = selected_topic_ids.merge(cleaned_sentiment_df, on='id', how='left')\n",
    "\n",
    "    # export selected topic sentiment to csv\n",
    "    filename = BTM_DATA_IN_PREFIX + f\"tweet_sentiment_subdf_topic_{selected_topic}.csv\"\n",
    "    selected_topic_sentiment_df.to_csv(filename)\n",
    "\n",
    "    return selected_topic_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5caa2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 40\n",
    "optimal_num_topics = 11\n",
    "df = load_data(optimal_num_topics)\n",
    "df = match_topic_with_tweet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b57d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_topic = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5617ee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGetting topic sentiment...\n",
      "\t\tGetting sentiment polarity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 100879/100879 [00:05<00:00, 18669.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSplitting polarity scores into columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 100879/100879 [00:07<00:00, 14015.79it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tGetting topic sentiment...\")\n",
    "# sentiment analysis\n",
    "df = sentiment_get_matching_topic_data(selected_topic)\n",
    "df = clean_sentiment_data(df)\n",
    "filename = SENTIMENT_DATA_OUT_PREFIX + f\"rounded_sentiment_topic_{selected_topic}.pdf\"\n",
    "\n",
    "df = sentiment_polarity_score(df, False, selected_topic, filename)\n",
    "# segments\n",
    "df, sub_dfs, num_segments = split_data_segments(df, NUM_SEGMENTS)\n",
    "num_tweets_per_segment = round(len(sub_dfs[0]) / 1000, 1)\n",
    "filename = SENTIMENT_DATA_OUT_PREFIX + f\"sentiment_per_segment_topic_{selected_topic}.pdf\"\n",
    "lt_compound_df, lt_num_segments, lt_num_tweets_per_segment, overall, selected_topic, filename = sentiment_per_segment(df, sub_dfs, num_segments, num_tweets_per_segment, False, selected_topic, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c07850",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_over_time(overall_compound_df, overall_num_segments, overall_num_tweets_per_segment, lt_compound_df, lt_num_segments, lt_num_tweets_per_segment, selected_topic, filename):\n",
    "    '''\n",
    "        Plot sentiment over time.\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(overall_compound_df.date, 'compouned', data=overall_compound_df, label=\"Overall\")\n",
    "    ax.plot(lt_compound_df.date, 'compouned', data=lt_compound_df, label=f\"Topic {selected_topic}\")\n",
    "\n",
    "    # Major ticks every month.\n",
    "    fmt_month = mdates.MonthLocator()\n",
    "    ax.xaxis.set_major_locator(fmt_month)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "    # plot\n",
    "    plt.title(f'Overall and Topic {selected_topic} Sentiment per segment')\n",
    "    \n",
    "    plt.legend(loc=\"lower left\")\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Vader Sentiment score')\n",
    "    \n",
    "#     plt.figtext(0.14,-0.05,f'Overall: {overall_num_segments} segments of ~{overall_num_tweets_per_segment}k, Topic {selected_topic}: {lt_num_segments} segments of ~{lt_num_tweets_per_segment}k')\n",
    "    # save graph\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7ae9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = SENTIMENT_DATA_OUT_PREFIX + f\"sentiment_per_segment_topic_overall_{selected_topic}.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d6511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_over_time(overall_compound_df, overall_num_segments, overall_num_tweets_per_segment, lt_compound_df, lt_num_segments, lt_num_tweets_per_segment, selected_topic, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab6ed76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>rounded_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28510</th>\n",
       "      <td>1356193045817872384</td>\n",
       "      <td>2021-02-01 10:49:53+00:00</td>\n",
       "      <td>a great project</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>10:49:53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28516</th>\n",
       "      <td>1356203583193063424</td>\n",
       "      <td>2021-02-01 11:31:45+00:00</td>\n",
       "      <td>$dena great</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>11:31:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28478</th>\n",
       "      <td>1356665511027875840</td>\n",
       "      <td>2021-02-02 18:07:18+00:00</td>\n",
       "      <td>thanks for opportunity</td>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>18:07:18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28485</th>\n",
       "      <td>1356802495968727040</td>\n",
       "      <td>2021-02-03 03:11:37+00:00</td>\n",
       "      <td>good project best nft</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>03:11:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28487</th>\n",
       "      <td>1356805106969153536</td>\n",
       "      <td>2021-02-03 03:22:00+00:00</td>\n",
       "      <td>best nft i like it</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>03:22:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28512</th>\n",
       "      <td>1399515472840724480</td>\n",
       "      <td>2021-05-31 23:57:45+00:00</td>\n",
       "      <td>amazing project. this project is going to be g...</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>23:57:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.9081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28511</th>\n",
       "      <td>1399515534023036928</td>\n",
       "      <td>2021-05-31 23:57:59+00:00</td>\n",
       "      <td>thanks for giving us such a great opportunity....</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>23:57:59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28507</th>\n",
       "      <td>1399515936093204480</td>\n",
       "      <td>2021-05-31 23:59:35+00:00</td>\n",
       "      <td>such a beautiful project and congratulations t...</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>23:59:35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.9677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>1399515957362450432</td>\n",
       "      <td>2021-05-31 23:59:40+00:00</td>\n",
       "      <td>great project! $reset</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>23:59:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.6588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121144</th>\n",
       "      <td>1399515966774530048</td>\n",
       "      <td>2021-05-31 23:59:42+00:00</td>\n",
       "      <td>good project</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>23:59:42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100879 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                created_at  \\\n",
       "28510   1356193045817872384 2021-02-01 10:49:53+00:00   \n",
       "28516   1356203583193063424 2021-02-01 11:31:45+00:00   \n",
       "28478   1356665511027875840 2021-02-02 18:07:18+00:00   \n",
       "28485   1356802495968727040 2021-02-03 03:11:37+00:00   \n",
       "28487   1356805106969153536 2021-02-03 03:22:00+00:00   \n",
       "...                     ...                       ...   \n",
       "28512   1399515472840724480 2021-05-31 23:57:45+00:00   \n",
       "28511   1399515534023036928 2021-05-31 23:57:59+00:00   \n",
       "28507   1399515936093204480 2021-05-31 23:59:35+00:00   \n",
       "9108    1399515957362450432 2021-05-31 23:59:40+00:00   \n",
       "121144  1399515966774530048 2021-05-31 23:59:42+00:00   \n",
       "\n",
       "                                            cleaned_tweet        date  \\\n",
       "28510                                     a great project  2021-02-01   \n",
       "28516                                         $dena great  2021-02-01   \n",
       "28478                              thanks for opportunity  2021-02-02   \n",
       "28485                               good project best nft  2021-02-03   \n",
       "28487                                  best nft i like it  2021-02-03   \n",
       "...                                                   ...         ...   \n",
       "28512   amazing project. this project is going to be g...  2021-05-31   \n",
       "28511   thanks for giving us such a great opportunity....  2021-05-31   \n",
       "28507   such a beautiful project and congratulations t...  2021-05-31   \n",
       "9108                                great project! $reset  2021-05-31   \n",
       "121144                                       good project  2021-05-31   \n",
       "\n",
       "            time  neg    neu    pos  compound  rounded_polarity  \n",
       "28510   10:49:53  0.0  0.328  0.672    0.6249                 1  \n",
       "28516   11:31:45  0.0  0.196  0.804    0.6249                 1  \n",
       "28478   18:07:18  0.0  0.149  0.851    0.6908                 1  \n",
       "28485   03:11:37  0.0  0.220  0.780    0.7964                 1  \n",
       "28487   03:22:00  0.0  0.309  0.691    0.7717                 1  \n",
       "...          ...  ...    ...    ...       ...               ...  \n",
       "28512   23:57:45  0.0  0.668  0.332    0.9081                 1  \n",
       "28511   23:57:59  0.0  0.444  0.556    0.9571                 1  \n",
       "28507   23:59:35  0.0  0.463  0.537    0.9677                 1  \n",
       "9108    23:59:40  0.0  0.313  0.687    0.6588                 1  \n",
       "121144  23:59:42  0.0  0.256  0.744    0.4404                 1  \n",
       "\n",
       "[100879 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
