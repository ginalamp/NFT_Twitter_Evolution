---
title: "BTM Implementation"
output: html_notebook
---

Using [BTM cran package](https://cran.r-project.org/web/packages/BTM/BTM.pdf), based on [this](https://github.com/bnosac/BTM) GitHub code.

```{r}
library(tidytext)
library(dplyr)
library(udpipe)
library(BTM) # topic modelling
library(data.table) # dataframe to datatable

# visualise model
library(textplot)
library(ggraph)
library(concaveman)
```

```{r}
df <- read.csv("../datain/topic_modelling/cleaned_tweets_largest_community_btm.csv")
df
```
```{r}
clean_tweets <- function(tweets_in) {
  # remove stopwords, links, mentions, special characters, and numbers
  tidy_tweets <- tweets_in %>% 
    unnest_tokens(word, cleaned_tweet, token = "tweets")

  return(tidy_tweets)
}

df <- clean_tweets(df)
```


data: a tokenised data frame containing one row per token with 2 columns

* the first column is a context identifier (e.g. a tweet id, a document id, a sentence id, an identifier of a survey answer, an identifier of a part of a text)

* the second column is a column called of type character containing the sequence of words occurring within the context identifier

```{r convert data frame to data table (set)}
# https://stackoverflow.com/questions/48758883/r-dataset-not-found
setDT(df)
df
```

```{r}
numTopics <- 12
numGibbsIter <- 1000
gibbsEvolution <- 100
```

```{r}
set.seed(1234)
model  <- BTM(data = df, k = numTopics, iter = numGibbsIter, trace = gibbsEvolution)
```

```{r}
# I think here is where the logLik code should go (after creating the model)
fit <- logLik(model)
fit$ll
```

```{r}
# theta: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it
model$theta
```
```{r}
# phi: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z). the rownames of the matrix indicate the token w
model$phi
```

```{r}
topicterms <- terms(model, top_n = 10)
topicterms
```


```{r}
scores <- predict(model, newdata = df)
scores
```
<!-- x -->
<!-- ```{r} -->
<!-- # If you set background to TRUE -->
<!-- # The first topic is set to a background topic that equals to the empirical word distribution.  -->
<!-- # This can be used to filter out common words. -->
<!-- set.seed(1234) -->
<!-- model      <- BTM(df, k = 12, beta = 0.01, background = TRUE, iter = 1000, trace = 100) -->
<!-- topicterms <- terms(model, top_n = 20) -->
<!-- topicterms -->
<!-- ``` -->
```{r}
plot(model)
```
# Write results to files
```{r}
# phi: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z). the rownames of the matrix indicate the token w
write.csv(x=model$phi, file=sprintf("dataout/%d_model_phi.csv", numTopics))

# a matrix containing containing P(z|d) - the probability of the topic given the biterms.
# The matrix has one row for each unique doc_id (context identifier) which contains words part of the dictionary of the BTM model and has K columns, one for each topic.
write.csv(x=scores, file=sprintf("dataout/%d_model_scores.csv", numTopics))

# theta: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it
write.csv(x=model$theta, file=sprintf("dataout/%d_model_theta.csv", numTopics))
```

```{r}
# alpha: the symmetric dirichlet prior probability of a topic P(z)
write.csv(x=model$alpha, file=sprintf("dataout/%d_model_alpha.csv", numTopics))

# beta: the symmetric dirichlet prior probability of a word given the topic P(w|z)
write.csv(x=model$beta, file=sprintf("dataout/%d_model_beta.csv", numTopics))
```








